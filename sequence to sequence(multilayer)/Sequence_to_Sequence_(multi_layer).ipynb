{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequence to Sequence (multi-layer)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC5A1C25f6Y-",
        "outputId": "ade65b80-ad8a-4b35-b5b2-35498eb7ddf7"
      },
      "source": [
        "!pip show tensorflow\r\n",
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.4.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: wrapt, h5py, termcolor, wheel, astunparse, opt-einsum, protobuf, gast, tensorflow-estimator, keras-preprocessing, typing-extensions, grpcio, tensorboard, numpy, google-pasta, six, absl-py, flatbuffers\n",
            "Required-by: fancyimpute\n",
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.19.4)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (51.0.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vnz7cD7zpNy"
      },
      "source": [
        "#from __future__ import unicode_literals, print_function, division\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import string\r\n",
        "from string import digits\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\r\n",
        "from keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, Flatten, Dropout\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Model, Sequential, load_model\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "from numpy import array, argmax\r\n",
        "from numpy.random import rand, shuffle\r\n",
        "from nltk.translate.bleu_score import corpus_bleu\r\n",
        "from nltk.translate.bleu_score import sentence_bleu\r\n",
        "import scipy\r\n",
        "import statsmodels\r\n",
        "import sklearn\r\n",
        "import tensorflow\r\n",
        "import keras\r\n",
        "from io import open\r\n",
        "import unicodedata\r\n",
        "import random\r\n",
        "import math\r\n",
        "import os\r\n",
        "import time\r\n",
        "import re\r\n",
        "import sys\r\n",
        "from unicodedata import normalize\r\n",
        "from numpy import array\r\n",
        "from pickle import dump, load\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.utils.vis_utils import plot_model\r\n",
        "from keras.layers import RepeatVector, TimeDistributed\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "\r\n",
        "INDIC_NLP_LIB_HOME=r\"/content/drive/MyDrive/anoopkunchukuttan-indic_nlp_library-eccde81/src\"\r\n",
        "INDIC_NLP_RESOURCES=r\"/content/drive/MyDrive/indic_nlp_resources-master\"\r\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\r\n",
        "from indicnlp import common\r\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\r\n",
        "from indicnlp import loader\r\n",
        "\r\n",
        "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\r\n",
        "from indicnlp.tokenize import indic_tokenize"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "Mh7NAWpjqARy",
        "outputId": "c119545f-800b-4007-f3a4-6efbd79aaef4"
      },
      "source": [
        "loader.load()\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cf842fd27528>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/anoopkunchukuttan-indic_nlp_library-eccde81/src/indicnlp/loader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m## Initialization of Indic scripts module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mindic_scripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m## Initialization of English scripts module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/anoopkunchukuttan-indic_nlp_library-eccde81/src/indicnlp/script/indic_scripts.py\u001b[0m in \u001b[0;36minit\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mTAMIL_PHONETIC_DATA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_resources_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'script'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tamil_script_phonetic_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mALL_PHONETIC_VECTORS\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mALL_PHONETIC_DATA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPHONETIC_VECTOR_START_OFFSET\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mTAMIL_PHONETIC_VECTORS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTAMIL_PHONETIC_DATA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPHONETIC_VECTOR_START_OFFSET\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'ix'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OKruUz9EFpb"
      },
      "source": [
        "def load_doc(filename):\r\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\r\n",
        "    text = file.read()\r\n",
        "    file.close()\r\n",
        "    return text\r\n",
        "\r\n",
        "def to_pairs(hindi_text, telugu_text):\r\n",
        "    hindi_lines = hindi_text.strip().split('\\n')\r\n",
        "    telugu_lines = telugu_text.strip().split('\\n')\r\n",
        "    pairs = []\r\n",
        "    for i in range(len(telugu_lines)):\r\n",
        "        pairs.append([])\r\n",
        "        pairs[i].append(pre_process_hindi_sentence(hindi_lines[i]))\r\n",
        "        pairs[i].append(pre_process_telugu_sentence(telugu_lines[i]))\r\n",
        "    return pairs\r\n",
        "\r\n",
        "def clean_text(text):\r\n",
        "    text = text.replace(u',','')\r\n",
        "    text = text.replace(u'\"','')\r\n",
        "    text = text.replace(u'\"','')\r\n",
        "    text = text.replace(u\"‘‘\",'')\r\n",
        "    text = text.replace(u\"’’\",'')\r\n",
        "    text = text.replace(u\"''\",'')\r\n",
        "    text = text.replace(u\"।\",'')\r\n",
        "    text=text.replace(u',','')\r\n",
        "    text=text.replace(u'\"','')\r\n",
        "    text=text.replace(u'(','')\r\n",
        "    text=text.replace(u')','')\r\n",
        "    text=text.replace(u'\"','')\r\n",
        "    text=text.replace(u':','')\r\n",
        "    text=text.replace(u\"'\",'')\r\n",
        "    text=text.replace(u\"‘‘\",'')\r\n",
        "    text=text.replace(u\"’’\",'')\r\n",
        "    text=text.replace(u\"''\",'')\r\n",
        "    text=text.replace(u\".\",'')\r\n",
        "    text=text.replace(u\"-\",'')\r\n",
        "    text=text.replace(u\"।\",'')\r\n",
        "    text=text.replace(u\"?\",'')\r\n",
        "    text=text.replace(u\"\\\\\",'')\r\n",
        "    text=text.replace(u\"_\",'')\r\n",
        "    text= re.sub(\"'\", '', text)\r\n",
        "    text=re.sub('[0-9+\\-*/.%]', '', text)\r\n",
        "    text=text.strip()\r\n",
        "    text=re.sub(' +', ' ',text)\r\n",
        "    exclude = set(string.punctuation)\r\n",
        "    text= ''.join(ch for ch in text if ch not in exclude)\r\n",
        "    return text\r\n",
        "\r\n",
        "def pre_process_hindi_sentence(line):\r\n",
        "    line=re.sub('[a-zA-Z]', '', line)\r\n",
        "    line = clean_text(line)\r\n",
        "    remove_nuktas = False\r\n",
        "    factory = IndicNormalizerFactory()\r\n",
        "    normalizer = factory.get_normalizer(\"hi\",remove_nuktas)\r\n",
        "    line = normalizer.normalize(line)\r\n",
        "    tokens = list()\r\n",
        "    for t in indic_tokenize.trivial_tokenize(line):\r\n",
        "        tokens.append(t)\r\n",
        "    line = tokens\r\n",
        "    line = [word for word in line if not re.search(r'\\d', word)]\r\n",
        "    line = ' '.join(line)\r\n",
        "    line = 'START_ '+ line + ' _END'\r\n",
        "    return (line)\r\n",
        "\r\n",
        "def pre_process_telugu_sentence(line):\r\n",
        "    line=re.sub('[a-zA-Z]', '', line)\r\n",
        "    line = clean_text(line)\r\n",
        "    remove_nuktas = False\r\n",
        "    factory = IndicNormalizerFactory()\r\n",
        "    normalizer = factory.get_normalizer(\"hi\",remove_nuktas)\r\n",
        "    line = normalizer.normalize(line)\r\n",
        "    tokens = list()\r\n",
        "    for t in indic_tokenize.trivial_tokenize(line):\r\n",
        "        tokens.append(t)\r\n",
        "    line = tokens\r\n",
        "    line = [word for word in line if not re.search(r'\\d', word)]\r\n",
        "    line = ' '.join(line)\r\n",
        "    line = 'START_ '+ line + ' _END'\r\n",
        "    return (line)\r\n",
        "\r\n",
        "hindi_text = load_doc('/content/drive/MyDrive/surya_hindi_telugu/HIN.txt')\r\n",
        "telugu_text = load_doc('/content/drive/MyDrive/surya_hindi_telugu/TEL.txt')\r\n",
        "pairs = to_pairs(hindi_text, telugu_text)\r\n",
        "df = pd.DataFrame(pairs)\r\n",
        "df.columns = [\"hin\", \"tel\"]\r\n",
        "#df"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMX5TPYQqmg7"
      },
      "source": [
        "lines = df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUhS04nwGN_n",
        "outputId": "2b3cd6dd-a3ca-45ae-fac6-53b59902afda"
      },
      "source": [
        "all_hin_words=set()\r\n",
        "for hin in lines.hin:\r\n",
        "    for word in hin.split():\r\n",
        "        if word not in all_hin_words:\r\n",
        "            all_hin_words.add(word)\r\n",
        "\r\n",
        "\r\n",
        "all_telugu_words=set()\r\n",
        "for tel in lines.tel:\r\n",
        "    for word in tel.split():\r\n",
        "        if word not in all_telugu_words:\r\n",
        "            all_telugu_words.add(word)\r\n",
        "\r\n",
        "#print (all_hin_words)\r\n",
        "print (len(all_hin_words))\r\n",
        "print (len(all_telugu_words))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12560\n",
            "16242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdawxgP4qmpR",
        "outputId": "465bc7d1-632a-45d6-e8ee-723a031a3493"
      },
      "source": [
        "# Max Length of source sequence\r\n",
        "lenght_list=[]\r\n",
        "for l in lines.hin:\r\n",
        "    lenght_list.append(len(l.split(' ')))\r\n",
        "max_length_src = np.max(lenght_list)\r\n",
        "print (max_length_src)\r\n",
        "\r\n",
        "# Max Length of target sequence\r\n",
        "lenght_list=[]\r\n",
        "for l in lines.tel:\r\n",
        "    lenght_list.append(len(l.split(' ')))\r\n",
        "max_length_tar = np.max(lenght_list)\r\n",
        "print (max_length_tar)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "85\n",
            "59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T90EKTSUqmrI",
        "outputId": "4e529379-0719-4a4f-cea5-1e244d367adf"
      },
      "source": [
        "input_words = sorted(list(all_hin_words))\r\n",
        "target_words = sorted(list(all_telugu_words))\r\n",
        "num_encoder_tokens = len(all_hin_words)\r\n",
        "num_decoder_tokens = len(all_telugu_words)\r\n",
        "num_encoder_tokens, num_decoder_tokens\r\n",
        "num_encoder_tokens += 1\r\n",
        "num_decoder_tokens += 1 # For zero padding\r\n",
        "num_decoder_tokens"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16243"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m_zEkjSqmvu"
      },
      "source": [
        "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\r\n",
        "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\r\n",
        "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\r\n",
        "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9HxYM9_qmx0",
        "outputId": "a1867ad3-dde5-4d5b-f806-29b779a6b452"
      },
      "source": [
        "# Train - Test Split\r\n",
        "X, y = lines.hin, lines.tel\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\r\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((9899,), (1100,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwM216u-qm3F"
      },
      "source": [
        "X_train.to_pickle('X_train.pkl')\r\n",
        "X_test.to_pickle('X_test.pkl')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkex7bd7qm5W"
      },
      "source": [
        "def generate_batch(X = X_train, y = y_train, batch_size = 128):\r\n",
        "    while True:\r\n",
        "        # range(start, stop, step)\r\n",
        "        for j in range(0, len(X), batch_size):\r\n",
        "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\r\n",
        "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\r\n",
        "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\r\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\r\n",
        "                for t, word in enumerate(input_text.split()):\r\n",
        "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\r\n",
        "                for t, word in enumerate(target_text.split()):\r\n",
        "                    if t<len(target_text.split())-1:\r\n",
        "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\r\n",
        "                    if t>0:\r\n",
        "                        # decoder target sequence (one hot encoded)\r\n",
        "                        # does not include the START_ token\r\n",
        "                        # Offset by one timestep\r\n",
        "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\r\n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmaXmRYDqm-U"
      },
      "source": [
        "latent_dim = 500\r\n",
        "vec_len = 300 # Length of the vector that we willl get from the embedding layer\r\n",
        "dropout_rate = 0.2 # Rate of the dropout layers\r\n",
        "encoder_inputs = Input(shape=(None,))\r\n",
        "\r\n",
        "#enc_emb =  Embedding(num_encoder_tokens, vec_len, mask_zero = True)(encoder_inputs)\r\n",
        "#enc_emb =  Embedding(num_encoder_tokens, vec_len)(encoder_inputs)\r\n",
        "enc_emb = Embedding(input_dim = num_encoder_tokens, output_dim = vec_len, mask_zero = True)(encoder_inputs)\r\n",
        "\r\n",
        "encoder_dropout = (TimeDistributed(Dropout(rate = dropout_rate)))(enc_emb)\r\n",
        "encoder_lstm1 = LSTM(latent_dim, return_sequences=True)(encoder_dropout)\r\n",
        "encoder_lstm2 = LSTM(latent_dim, return_sequences=True)(encoder_lstm1)\r\n",
        "encoder_lstm3 = LSTM(latent_dim, return_sequences=True)(encoder_lstm2)\r\n",
        "encoder_LSTM4_layer = LSTM(latent_dim, return_state=True)\r\n",
        "encoder_outputs, state_h, state_c = encoder_LSTM4_layer(encoder_lstm3)\r\n",
        "# We discard `encoder_outputs` and only keep the states.\r\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOqrj2FqqnBQ"
      },
      "source": [
        "# Set up the decoder, using `encoder_states` as initial state.\r\n",
        "decoder_inputs = Input(shape=(None,))\r\n",
        "dec_emb_layer = Embedding(num_decoder_tokens, vec_len, mask_zero = True)\r\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\r\n",
        "decoder_dropout = (TimeDistributed(Dropout(rate = dropout_rate)))(dec_emb)\r\n",
        "# We set up our decoder to return full output sequences,\r\n",
        "# and to return internal states as well. We don't use the\r\n",
        "# return states in the training model, but we will use them in inference.\r\n",
        "\r\n",
        "\r\n",
        "decoder_LSTM_layer = LSTM(latent_dim, return_sequences=True)\r\n",
        "decoder_LSTM = decoder_LSTM_layer(decoder_dropout, initial_state = encoder_states)\r\n",
        "\r\n",
        "decoder_LSTM_2_layer = LSTM(latent_dim, return_sequences=True, return_state=True)\r\n",
        "decoder_outputs,_,_ = decoder_LSTM_2_layer(decoder_LSTM)\r\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\r\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\r\n",
        "\r\n",
        "# Define the model that will turn\r\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\r\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lBiTCdzqnD-",
        "outputId": "871f1b4a-0ea8-48ac-c7a7-8319ab37af85"
      },
      "source": [
        "model.summary()\r\n",
        "\r\n",
        "# Define a checkpoint callback :\r\n",
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \r\n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\r\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 300)    3768300     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 300)    0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, None, 500)    1602000     time_distributed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, None, 500)    2002000     lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 300)    4872900     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, None, 500)    2002000     lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, None, 300)    0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, 500), (None, 2002000     lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   (None, None, 500)    1602000     time_distributed_1[0][0]         \n",
            "                                                                 lstm_3[0][1]                     \n",
            "                                                                 lstm_3[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   [(None, None, 500),  2002000     lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 16243)  8137743     lstm_5[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 27,990,943\n",
            "Trainable params: 27,990,943\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJCNNRDcqnIa",
        "outputId": "878d3e16-3539-455f-e31a-dbe5cff394de"
      },
      "source": [
        "train_samples = len(X_train)\r\n",
        "val_samples = len(X_test)\r\n",
        "batch_size = 80\r\n",
        "epochs = 10\r\n",
        "\r\n",
        "# Compile the model\r\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\r\n",
        "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\r\n",
        "                    steps_per_epoch = train_samples//batch_size,\r\n",
        "                    epochs=epochs,\r\n",
        "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\r\n",
        "                    validation_steps = val_samples//batch_size)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "123/123 [==============================] - 1757s 14s/step - loss: 1.5797 - acc: 0.0906 - val_loss: 1.4879 - val_acc: 0.1107\n",
            "Epoch 2/10\n",
            "123/123 [==============================] - 1729s 14s/step - loss: 1.4203 - acc: 0.1099 - val_loss: 1.4754 - val_acc: 0.1124\n",
            "Epoch 3/10\n",
            "123/123 [==============================] - 1713s 14s/step - loss: 1.3720 - acc: 0.1131 - val_loss: 1.4639 - val_acc: 0.1168\n",
            "Epoch 4/10\n",
            "123/123 [==============================] - 1707s 14s/step - loss: 1.3347 - acc: 0.1184 - val_loss: 1.4555 - val_acc: 0.1157\n",
            "Epoch 5/10\n",
            "123/123 [==============================] - 1705s 14s/step - loss: 1.2969 - acc: 0.1240 - val_loss: 1.4524 - val_acc: 0.1206\n",
            "Epoch 6/10\n",
            "123/123 [==============================] - 1708s 14s/step - loss: 1.2612 - acc: 0.1275 - val_loss: 1.4291 - val_acc: 0.1249\n",
            "Epoch 7/10\n",
            "123/123 [==============================] - 1718s 14s/step - loss: 1.2251 - acc: 0.1325 - val_loss: 1.4275 - val_acc: 0.1255\n",
            "Epoch 8/10\n",
            "123/123 [==============================] - 1718s 14s/step - loss: 1.1830 - acc: 0.1391 - val_loss: 1.4298 - val_acc: 0.1254\n",
            "Epoch 9/10\n",
            "123/123 [==============================] - 1712s 14s/step - loss: 1.1437 - acc: 0.1454 - val_loss: 1.4343 - val_acc: 0.1243\n",
            "Epoch 10/10\n",
            "123/123 [==============================] - 1707s 14s/step - loss: 1.1125 - acc: 0.1518 - val_loss: 1.4227 - val_acc: 0.1263\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe26bac9b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Tk3-bGTUjY2"
      },
      "source": [
        "#save the model\r\n",
        "model.save_weights('nmt_weights.h5')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge-GWNCmUjlD"
      },
      "source": [
        "#load the model\r\n",
        "model.load_weights('nmt_weights.h5')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOrIxtwAUjoE"
      },
      "source": [
        "# Encode the input sequence to get the \"thought vectors\"\r\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\r\n",
        "\r\n",
        "# Decoder setup\r\n",
        "# Below tensors will hold the states of the previous time step\r\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\r\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\r\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\r\n",
        "\r\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\r\n",
        "decoder_dropout2 = (TimeDistributed(Dropout(rate = dropout_rate)))(dec_emb2)\r\n",
        "decoder_LSTM2 = decoder_LSTM_layer(decoder_dropout2, initial_state = decoder_states_inputs)\r\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_LSTM_2_layer(decoder_LSTM2)\r\n",
        "decoder_states2 = [state_h2, state_c2]\r\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2) #A dense softmax layer to generate prob dist. over the target vocabulary\r\n",
        "\r\n",
        "\r\n",
        "# Final decoder model\r\n",
        "decoder_model = Model(\r\n",
        "    [decoder_inputs] + decoder_states_inputs,\r\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X81YQpIFUjsq"
      },
      "source": [
        "def decode_sequence(input_seq):\r\n",
        "    # Encode the input as state vectors.\r\n",
        "    states_value = encoder_model.predict(input_seq)\r\n",
        "    # Generate empty target sequence of length 1.\r\n",
        "    target_seq = np.zeros((1,1))\r\n",
        "    # Populate the first character of target sequence with the start character.\r\n",
        "    target_seq[0, 0] = target_token_index['START_']\r\n",
        "\r\n",
        "    # Sampling loop for a batch of sequences\r\n",
        "    # (to simplify, here we assume a batch of size 1).\r\n",
        "    stop_condition = False\r\n",
        "    decoded_sentence = ''\r\n",
        "    while not stop_condition:\r\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\r\n",
        "\r\n",
        "        # Sample a token\r\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\r\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\r\n",
        "        decoded_sentence += ' '+sampled_char\r\n",
        "\r\n",
        "        # Exit condition: either hit max length\r\n",
        "        # or find stop character.\r\n",
        "        if (sampled_char == '_END' or\r\n",
        "           len(decoded_sentence) > 98):\r\n",
        "            stop_condition = True\r\n",
        "\r\n",
        "        # Update the target sequence (of length 1).\r\n",
        "        target_seq = np.zeros((1,1))\r\n",
        "        target_seq[0, 0] = sampled_token_index\r\n",
        "\r\n",
        "        # Update states\r\n",
        "        states_value = [h, c]\r\n",
        "\r\n",
        "    return decoded_sentence"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udiKXgMiUjxY"
      },
      "source": [
        "train_gen = generate_batch(X_train, y_train, batch_size = 1)\r\n",
        "k=-1"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Dch_MIMUj4F",
        "outputId": "401f0b00-40fa-45e5-c9c2-491a37a803d0"
      },
      "source": [
        "k+=1\r\n",
        "(input_seq, actual_output), _ = next(train_gen)\r\n",
        "decoded_sentence = decode_sequence(input_seq)\r\n",
        "print('Input hindi sentence:', X_train[k:k+1].values[0])\r\n",
        "print('Actual telugu Translation:', y_train[k:k+1].values[0][6:-4])\r\n",
        "print('Predicted telugu Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input hindi sentence: START_ इस मौसम की आर्द्रता और तापमान जीवाणुओं और कीटाणुओं को पनपने में सहायक होते है _END\n",
            "Actual telugu Translation:  ఈ వాతావరణం యొక్క తేమ మరియు ఉష్ణోగ్రత జీవులు మరియు కీటకాల పెరుగుదలకు సహాయపడుతుంది \n",
            "Predicted telugu Translation:  ఈ వ్యాధి తీసుకెళ్లండి \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFiemFwxUj67",
        "outputId": "3f05be88-dc25-4cfe-b971-4f83a2fcb259"
      },
      "source": [
        "k+=1\r\n",
        "(input_seq, actual_output), _ = next(train_gen)\r\n",
        "decoded_sentence = decode_sequence(input_seq)\r\n",
        "print('Input hindi sentence:', X_train[k:k+1].values[0])\r\n",
        "print('Actual telugu Translation:', y_train[k:k+1].values[0][6:-4])\r\n",
        "print('Predicted telugu Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input hindi sentence: START_ औरत ऑपरेशन के दौरान पूरे होश में रहती है _END\n",
            "Actual telugu Translation:  ఆపరేషన్ సమయంలో మహిళ పూర్తి స్పృహలో ఉంటుంది \n",
            "Predicted telugu Translation:  అందువల్ల వ్యాధి అర్ధగోళంలో కెరాటోమలాసియా ఈ వ్యాధి తీసుకెళ్లండి \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Umu5kuZrUkAM",
        "outputId": "11cd189c-337d-462d-dea4-d74b39383ebb"
      },
      "source": [
        "print('Input hindi sentence:', X_train[k:k+1].values[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input English sentence: START_ చర్న్ పురుగులు ఇవి తెల్లని తీగ లాగా చాలా చిన్నవి _END\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1Hdem9KUkFp",
        "outputId": "8ae05168-34b1-4536-8ac7-c95593e6509b"
      },
      "source": [
        "k+=1\r\n",
        "(input_seq, actual_output), _ = next(train_gen)\r\n",
        "decoded_sentence = decode_sequence(input_seq)\r\n",
        "print('Input hindi sentence:', X_train[k:k+1].values[0])\r\n",
        "print('Actual telugu Translation:', y_train[k:k+1].values[0][6:-4])\r\n",
        "print('Predicted telugu Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input hindi sentence: START_ मिलावटी चीजों से न सिर्फ लीवर बल्कि आँतों पर भी असर पड़ता है _END\n",
            "Actual telugu Translation:  కలుషితమైన విషయాలతో కాలేయం మాత్రమే కాదు దాని ప్రభావం ప్రేగులపై కూడా కనిపిస్తుంది \n",
            "Predicted telugu Translation:  మీరు ఆరోగ్య ఇవే ఈ వ్యాధి తీసుకెళ్లండి \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi2U1SpLKEUe",
        "outputId": "b695b75f-500c-4b2e-d8bc-e6a79723f6b1"
      },
      "source": [
        "k+=1\r\n",
        "(input_seq, actual_output), _ = next(train_gen)\r\n",
        "decoded_sentence = decode_sequence(input_seq)\r\n",
        "print('Input hindi sentence:', X_train[k:k+1].values[0])\r\n",
        "print('Actual telugu Translation:', y_train[k:k+1].values[0][6:-4])\r\n",
        "print('Predicted telugu Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input hindi sentence: START_ कैंसर के कारण ऐसे अनेक लक्षण दिखलाई पड़ने लगते हैं जो कैंसर होने का आभास देते हैं _END\n",
            "Actual telugu Translation:  క్యాన్సర్ కారణంగా అనేక లక్షణాలు కనిపిస్తాయి ఇవి క్యాన్సర్ యొక్క ముద్రలను ఇస్తాయి \n",
            "Predicted telugu Translation:  ఈ వ్యాధి తీసుకెళ్లండి \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGGiQWkrKEe-",
        "outputId": "5ae594a5-8780-44fb-f47c-2cb22c491675"
      },
      "source": [
        "k+=1\r\n",
        "(input_seq, actual_output), _ = next(train_gen)\r\n",
        "decoded_sentence = decode_sequence(input_seq)\r\n",
        "print('Input hindi sentence:', X_train[k:k+1].values[0])\r\n",
        "print('Actual telugu Translation:', y_train[k:k+1].values[0][6:-4])\r\n",
        "print('Predicted telugu Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input hindi sentence: START_ बच्चों के संपूर्ण आहार में कुछ चीजें जैसे फल दूध स्वीट्स सब्जियाँ अगर माँसाहारी है तो मीट अंडे बहुत महत्वपूर्ण हैं _END\n",
            "Actual telugu Translation:  పిల్లల పూర్తి ఆహారంలో పండ్లు పాలు స్వీట్లు కూరగాయలు మాంసాహారం ఉంటే మాంసం గుడ్లు చాలా ముఖ్యమైనవి \n",
            "Predicted telugu Translation:  మీరు ఆరోగ్య ఇవే ఈ వ్యాధి తీసుకెళ్లండి \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTzCKJFAKEhU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da50251-fcd8-49ea-f458-c754d71cf1c3"
      },
      "source": [
        "k+=1\r\n",
        "(input_seq, actual_output), _ = next(train_gen)\r\n",
        "decoded_sentence = decode_sequence(input_seq)\r\n",
        "print('Input hindi sentence:', X_train[k:k+1].values[0])\r\n",
        "print('Actual telugu Translation:', y_train[k:k+1].values[0][6:-4])\r\n",
        "print('Predicted telugu Translation:', decoded_sentence[:-4])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input hindi sentence: START_ मात्र रसौली की उपस्थिति ही उसे निकाले जाने की अनिवार्यता नहीं है _END\n",
            "Actual telugu Translation:  వాపు గ్రంథుల ఉనికి మాత్రమే దాని తొలగింపుకు అవసరం లేదు \n",
            "Predicted telugu Translation:  మీరు ఆరోగ్య ఇవే ఈ వ్యాధి తీసుకెళ్లండి \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc2Sox-wKEmH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrNMbwpxKEoi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDg98740KEs9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4go50oVMKEvZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIX4Am2-KEzJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}